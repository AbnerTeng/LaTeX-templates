\documentclass[12pt]{article}
\usepackage{geometry} 
\usepackage{amsmath,amsthm,amssymb,scrextend}
\usepackage{fancyhdr}
\setlength{\headheight}{14.5pt}
\addtolength{\topmargin}{-2.5pt}
\pagestyle{fancy}

\newcommand{\cont}{\subseteq}
\usepackage{tikz}
\usepackage{pgfplots}
\usepackage{amsmath}
\usepackage[mathscr]{euscript}
\let\euscr\mathscr\let\mathscr\relax% just so we can load this and rsfs
\usepackage[scr]{rsfso}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{multicol}
\usepackage[colorlinks=true, pdfstartview=FitV, linkcolor=blue,
citecolor=blue, urlcolor=blue]{hyperref}
\usepackage{graphicx}
\usepackage{float}
\usepackage{listings}
\usepackage{color}
\usepackage{xcolor}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\lstset{
			backgroundcolor=\color{red!50!green!50!blue!50!white!20},
			breaklines = true,
			tabsize=2,
			commentstyle=\color{codegreen},
			keywordstyle=\color{magenta},
			numberstyle=\tiny\color{codegray},
			stringstyle=\color{codepurple},
	    }
\DeclareMathOperator{\arcsec}{arcsec}
\DeclareMathOperator{\arccot}{arccot}
\DeclareMathOperator{\arccsc}{arccsc}
\newcommand{\ddx}{\frac{d}{dx}}
\newcommand{\dfdx}{\frac{df}{dx}}
\newcommand{\ddxp}[1]{\frac{d}{dx}\left(#1 \right)}
\newcommand{\dydx}{\frac{dy}{dx}}
\newcommand{\intx}[1]{\int#1 \, dx}
\newcommand{\intt}[1]{\int#1 \, dt}
\newcommand{\defint}[3]{\int_{#1}^{#2} #3 \, dx}
\newcommand{\imp}{\Rightarrow}
\newcommand{\un}{\cup}
\newcommand{\inter}{\cap}
\newcommand{\ps}{\mathscr{P}}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newtheorem*{sol}{Solution}
\newtheorem*{claim}{Claim}
\newtheorem{problem}{Problem}
\pgfplotsset{compat=1.17}
\begin{document}
 
% Don't change the above session

\lhead{22 AI-BD/problem 7.}
\chead{107205008, 109102057}
\rhead{\today}
 
% \maketitle
\section*{Problem 1.}
Train with 1,000 samples with \textbf{mnist}
\begin{lstlisting}[language = Matlab]
traindataset = datastore('mnist_train.csv')
trainfile =readall(traindataset)
testdata = datastore('mnist_test.csv')
testfile = readall(testdata)
dataset = [trainfile; testfile] 

knnmodel = fitcknn(dataset(1:1000, :), dataset.label(1:1000, :), NumNeighbors=10) 
predictions = predict(knnmodel, dataset(1001:end, :)); 
accuracy = sum(predictions == dataset.label(1001:end, :))/numel(predictions) 

neighborMatrix = zeros(20,2)
for i =1:20
    knnmodel = fitcknn(dataset(1:1000, :), dataset.label(1:1000, :), NumNeighbors=i);
    predictions = predict(knnmodel, dataset(1001:end, :));
    accuracy = sum(predictions == dataset.label(1001:end, :))/numel(predictions);
    temp = [i accuracy]
    neighborMatrix(i,1) = i;
    neighborMatrix(i,2) = accuracy;
end
    \end{lstlisting}

\section*{Problem 2.}
Evaluate with test samplesof original datasets
\[(1, 0.8692), (3, 0.8603)\]
$\bullet$ If i = 1, accuracy = 0.8692. \\
$\bullet$ IF i = 3, accuracy = 0.8603.

\section*{Problem 3.}
Tune up the parameter and choose your `best' model based on their performance.
\begin{lstlisting}[language = Matlab]
%% neural network
nnmodel = fitcnet(dataset(1:1000, :), dataset.label(1:1000, :))
predictions = predict(nnmodel, dataset(1001:end, :));
accuracy = sum(predictions == dataset.label(1001:end, :))/numel(predictions) %0.2071

%% decision tree
treemodel = fitctree(dataset(1:1000, :), dataset.label(1:1000, :))
predictions = predict(treemodel, dataset(1001:end, :));
accuracy = sum(predictions == dataset.label(1001:end, :))/numel(predictions) %1
\end{lstlisting}
$\bullet$ Dicision Tree Model is the best training model for this dataset.
    
\end{document}
